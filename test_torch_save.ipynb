{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using device: cuda\n",
      "{\n",
      "  \"Batch size\": 1024,\n",
      "  \"Parameter embedding dimension\": 64,\n",
      "  \"# of FNO modes\": 4,\n",
      "  \"# of FNO hidden channels\": 128,\n",
      "  \"# of FNO layers\": 3,\n",
      "  \"# of shared output channels\": 128,\n",
      "  \"Learning rate\": \"1.0e-03\"\n",
      "}\n",
      "Training set size: 944\n",
      "Validation set size: 202\n",
      "Test set size: 204\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import torch\n",
    "import numpy as np\n",
    "import h5py\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# from torch.serialization import add_safe_globals\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import TensorDataset, DataLoader, random_split\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "import neuralop as nop\n",
    "# from neuralop.models import FNO\n",
    "from fno_what import FNO\n",
    "\n",
    "\n",
    "# from neuralop.layers.spectral_convolution import SpectralConv\n",
    "\n",
    "# add_safe_globals([torch._C._nn.gelu])\n",
    "# add_safe_globals([nop.layers.spectral_convolution.SpectralConv])\n",
    "\n",
    "# 1. Load dataset.mat files\n",
    "data_dir = 'dataset/data/tapered_seal'\n",
    "mat_file = os.path.join(data_dir, '20250812_T_113003', 'dataset.mat')\n",
    "\n",
    "# 파라미터 설정\n",
    "batch_size = 2**10\n",
    "criterion = nop.losses.LpLoss(d=1, p=2)\n",
    "epochs = 2000\n",
    "param_embedding_dim = 64\n",
    "fno_modes = 4\n",
    "fno_hidden_channels = 128\n",
    "n_layers = 3\n",
    "shared_out_channels = fno_hidden_channels\n",
    "lr = 1e-3\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "print(f\"Using device: {device}\")\n",
    "weight_decay=1e-4\n",
    "\n",
    "import json\n",
    "\n",
    "hyperparams = {\n",
    "    \"Batch size\": batch_size,\n",
    "    \"Parameter embedding dimension\": param_embedding_dim,\n",
    "    \"# of FNO modes\": fno_modes,\n",
    "    \"# of FNO hidden channels\": fno_hidden_channels,\n",
    "    \"# of FNO layers\": n_layers,\n",
    "    \"# of shared output channels\": shared_out_channels,\n",
    "    \"Learning rate\": f\"{lr:.1e}\"\n",
    "}\n",
    "\n",
    "print(json.dumps(hyperparams, indent=2))\n",
    "\n",
    "# 데이터 로딩 및 전처리\n",
    "with h5py.File(mat_file, 'r') as mat:\n",
    "    # inputNond: [nPara, nData] 형상 파라미터\n",
    "    input_nond = np.array(mat.get('inputNond'))\n",
    "    # wVec: [1, nVel] 회전 속도 벡터 (좌표 그리드)\n",
    "    w_vec = np.array(mat['params/wVec'])\n",
    "    # RDC: [6, nVel, nData] 동특성 계수 (타겟 함수)\n",
    "    rdc = np.array(mat.get('RDC'))\n",
    "\n",
    "    n_para, n_data = input_nond.shape\n",
    "    _, n_vel = w_vec.shape\n",
    "    n_rdc_coeffs = rdc.shape[0] # 6 (K, k, C, c, M, m)\n",
    "\n",
    "    # 입력 데이터 (X): 형상 파라미터 [nData, nPara]\n",
    "    X_params = input_nond.T\n",
    "\n",
    "    # 출력 데이터 (y): 동특성 계수 함수 [nData, nVel, nRDC]\n",
    "    # FNO는 (batch, channels, grid_points) 형태를 선호하므로 [nData, nRDC, nVel]로 변경이라고 GPT가 그럔다\n",
    "    y_functions = rdc.transpose(2, 0, 1) # [nData, nRDC, nVel]\n",
    "\n",
    "    # 회전 속도 그리드: [nVel, 1]\n",
    "    \n",
    "    w = w_vec.squeeze()                         # [n_vel]\n",
    "    w_norm = 2 * (w - w.min()) / (w.max()-w.min()) - 1.0 # normalization\n",
    "    grid = w_norm[:, None] # [nVel, 1]\n",
    "\n",
    "indices = np.arange(n_data)\n",
    "train_size = int(n_data*0.7); val_size = int(n_data*0.15)\n",
    "test_size = n_data - train_size - val_size\n",
    "train_idx, val_idx, test_idx = np.split(np.random.permutation(indices),\n",
    "                                        [train_size, train_size+val_size])\n",
    "\n",
    "scaler_X = StandardScaler().fit(X_params[train_idx])\n",
    "X_scaled = np.empty_like(X_params, dtype=float)\n",
    "X_scaled[train_idx] = scaler_X.transform(X_params[train_idx])\n",
    "X_scaled[val_idx]  = scaler_X.transform(X_params[val_idx])\n",
    "X_scaled[test_idx] = scaler_X.transform(X_params[test_idx])\n",
    "\n",
    "scalers_y = [StandardScaler().fit(y_functions[train_idx, i, :].reshape(-1,1))\n",
    "             for i in range(n_rdc_coeffs)]\n",
    "\n",
    "y_scaled = np.empty_like(y_functions, dtype=float)\n",
    "for i in range(n_rdc_coeffs):\n",
    "    for split_idx in (train_idx, val_idx, test_idx):\n",
    "        y_scaled[split_idx, i, :] = scalers_y[i].transform(\n",
    "            y_functions[split_idx, i, :].reshape(-1,1)\n",
    "        ).reshape(-1, y_functions.shape[-1])\n",
    "\n",
    "# Torch 텐서로 변환\n",
    "X_tensor = torch.tensor(X_scaled, dtype=torch.float32)\n",
    "y_tensor = torch.tensor(y_scaled, dtype=torch.float32)\n",
    "grid_tensor = torch.tensor(grid, dtype=torch.float32)\n",
    "\n",
    "# 데이터셋 및 데이터로더 생성\n",
    "dataset = TensorDataset(X_tensor, y_tensor)\n",
    "\n",
    "dataset_size = len(dataset)\n",
    "train_size = int(dataset_size * 0.7)\n",
    "val_size = int(dataset_size * 0.15)\n",
    "test_size = dataset_size - train_size - val_size\n",
    "\n",
    "train_dataset, val_dataset, test_dataset = random_split(dataset, [train_size, val_size, test_size])\n",
    "\n",
    "print(f\"Training set size: {len(train_dataset)}\")\n",
    "print(f\"Validation set size: {len(val_dataset)}\")\n",
    "print(f\"Test set size: {len(test_dataset)}\")\n",
    "\n",
    "train_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True)\n",
    "val_loader = DataLoader(val_dataset, batch_size=batch_size, shuffle=False)\n",
    "test_loader = DataLoader(test_dataset, batch_size=batch_size, shuffle=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ParametricFNO(nn.Module):\n",
    "    \"\"\"\n",
    "    기존: 형상 파라미터를 조건으로 받아 함수를 예측하는 FNO 모델 (단일 네트워크)\n",
    "    outputs: [B, out_channels, n_vel]\n",
    "    \"\"\"\n",
    "    def __init__(self, n_params, param_embedding_dim, fno_modes, fno_hidden_channels, in_channels, out_channels,n_layers):\n",
    "        super().__init__()\n",
    "        self.n_params = n_params\n",
    "        self.param_encoder = nn.Sequential(\n",
    "            nn.Linear(n_params, param_embedding_dim),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(param_embedding_dim, param_embedding_dim)\n",
    "        )\n",
    "        self.fno = FNO(\n",
    "            n_modes=(fno_modes,),\n",
    "            hidden_channels=fno_hidden_channels,\n",
    "            n_layers=n_layers,\n",
    "            in_channels=in_channels + param_embedding_dim,\n",
    "            out_channels=out_channels\n",
    "        )\n",
    "\n",
    "    def forward(self, params, grid):\n",
    "        # params: [B, n_params], grid: [B, n_vel, 1]\n",
    "        pe = self.param_encoder(params)                      # [B, emb]\n",
    "        pe = pe.unsqueeze(1).repeat(1, grid.shape[1], 1)    # [B, n_vel, emb]\n",
    "        fno_in = torch.cat([grid, pe], dim=-1).permute(0, 2, 1)  # [B, 1+emb, n_vel]\n",
    "        out = self.fno(fno_in)  # [B, out_channels, n_vel]\n",
    "        return out\n",
    "    \n",
    "class MultiHeadParametricFNO(nn.Module):\n",
    "    \"\"\"\n",
    "    FNO 본체는 공유하고, 채널별 1x1 Conv1d 헤드를 분리하는 멀티헤드 구조.\n",
    "    outputs: [B, n_heads(=n_rdc_coeffs), n_vel]\n",
    "    \"\"\"\n",
    "    def __init__(self, n_params, param_embedding_dim, fno_modes, fno_hidden_channels, in_channels, n_heads,n_layers, shared_out_channels):\n",
    "        super().__init__()\n",
    "        self.n_params = n_params\n",
    "        self.param_encoder = nn.Sequential(\n",
    "            nn.Linear(n_params, param_embedding_dim),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(param_embedding_dim, param_embedding_dim)\n",
    "        )\n",
    "        self.trunk = FNO(\n",
    "            n_modes=(fno_modes,),\n",
    "            hidden_channels=fno_hidden_channels,\n",
    "            n_layers=n_layers,\n",
    "            in_channels=in_channels + param_embedding_dim,\n",
    "            out_channels=shared_out_channels\n",
    "        )\n",
    "        \n",
    "        self.heads = nn.ModuleList([\n",
    "            nn.Sequential(\n",
    "                nn.Conv1d(shared_out_channels, shared_out_channels, 1),\n",
    "                nn.GELU(),\n",
    "                nn.BatchNorm1d(shared_out_channels),\n",
    "                nn.Dropout(0.1),\n",
    "                # depth 2\n",
    "                nn.Conv1d(shared_out_channels, shared_out_channels // 2, 1),\n",
    "                nn.GELU(),\n",
    "                nn.BatchNorm1d(shared_out_channels // 2),\n",
    "                nn.Dropout(0.1),\n",
    "                # output\n",
    "                nn.Conv1d(shared_out_channels // 2, 1, 1)\n",
    "            ) for _ in range(n_heads)\n",
    "        ])\n",
    "\n",
    "    def forward(self, params, grid):\n",
    "        pe = self.param_encoder(params)                       # [B, emb]\n",
    "        pe = pe.unsqueeze(1).repeat(1, grid.shape[1], 1)     # [B, n_vel, emb]\n",
    "        x = torch.cat([grid, pe], dim=-1).permute(0, 2, 1)   # [B, 1+emb, n_vel]\n",
    "        feat = self.trunk(x)                                  # [B, Csh, n_vel]\n",
    "        outs = [head(feat) for head in self.heads]            # each: [B,1,n_vel]\n",
    "        return torch.cat(outs, dim=1)                         # [B, n_heads, n_vel]\n",
    "\n",
    "optimizer = None\n",
    "best_val_loss = float('inf')\n",
    "base_dir = 'test_dir'\n",
    "os.makedirs(base_dir, exist_ok=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<bound method Module.load_state_dict of MultiHeadParametricFNO(\n",
      "  (param_encoder): Sequential(\n",
      "    (0): Linear(in_features=3, out_features=64, bias=True)\n",
      "    (1): ReLU()\n",
      "    (2): Linear(in_features=64, out_features=64, bias=True)\n",
      "  )\n",
      "  (trunk): FNO(\n",
      "    (positional_embedding): GridEmbeddingND()\n",
      "    (fno_blocks): FNOBlocks(\n",
      "      (convs): ModuleList(\n",
      "        (0-2): 3 x SpectralConv(\n",
      "          (weight): DenseTensor(shape=torch.Size([128, 128, 3]), rank=None)\n",
      "        )\n",
      "      )\n",
      "      (fno_skips): ModuleList(\n",
      "        (0-2): 3 x Flattened1dConv(\n",
      "          (conv): Conv1d(128, 128, kernel_size=(1,), stride=(1,), bias=False)\n",
      "        )\n",
      "      )\n",
      "      (channel_mlp): ModuleList(\n",
      "        (0-2): 3 x ChannelMLP(\n",
      "          (fcs): ModuleList(\n",
      "            (0): Conv1d(128, 64, kernel_size=(1,), stride=(1,))\n",
      "            (1): Conv1d(64, 128, kernel_size=(1,), stride=(1,))\n",
      "          )\n",
      "        )\n",
      "      )\n",
      "      (channel_mlp_skips): ModuleList(\n",
      "        (0-2): 3 x SoftGating()\n",
      "      )\n",
      "    )\n",
      "    (lifting): ChannelMLP(\n",
      "      (fcs): ModuleList(\n",
      "        (0): Conv1d(66, 256, kernel_size=(1,), stride=(1,))\n",
      "        (1): Conv1d(256, 128, kernel_size=(1,), stride=(1,))\n",
      "      )\n",
      "    )\n",
      "    (projection): ChannelMLP(\n",
      "      (fcs): ModuleList(\n",
      "        (0): Conv1d(128, 256, kernel_size=(1,), stride=(1,))\n",
      "        (1): Conv1d(256, 128, kernel_size=(1,), stride=(1,))\n",
      "      )\n",
      "    )\n",
      "  )\n",
      "  (heads): ModuleList(\n",
      "    (0-5): 6 x Sequential(\n",
      "      (0): Conv1d(128, 128, kernel_size=(1,), stride=(1,))\n",
      "      (1): GELU(approximate='none')\n",
      "      (2): BatchNorm1d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (3): Dropout(p=0.1, inplace=False)\n",
      "      (4): Conv1d(128, 64, kernel_size=(1,), stride=(1,))\n",
      "      (5): GELU(approximate='none')\n",
      "      (6): BatchNorm1d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (7): Dropout(p=0.1, inplace=False)\n",
      "      (8): Conv1d(64, 1, kernel_size=(1,), stride=(1,))\n",
      "    )\n",
      "  )\n",
      ")>\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'\\nneuralop.models.base_model > BaseModel\\nBaseModel에 _metadata가 들어있어서 load_dict 할때 오류가 나는 것이다.\\n'"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model = MultiHeadParametricFNO(\n",
    "    n_params=n_para,\n",
    "    param_embedding_dim=param_embedding_dim,\n",
    "    fno_modes=fno_modes,\n",
    "    fno_hidden_channels=fno_hidden_channels,\n",
    "    in_channels=1,\n",
    "    n_heads=n_rdc_coeffs,\n",
    "    n_layers=n_layers,\n",
    "    shared_out_channels=shared_out_channels\n",
    ")\n",
    "model_save_path = os.path.join(base_dir, 'fno_seal_best_multihead.pth')\n",
    "torch.save(model.state_dict(), model_save_path)\n",
    "\n",
    "# model_save_path_script = os.path.join(base_dir, 'fno_seal_best_multihead_scripted.pt')\n",
    "# model_scripted = torch.jit.script(model)\n",
    "# model_scripted.save(model_save_path_script)\n",
    "\n",
    "print(model.load_state_dict)\n",
    "\n",
    "\"\"\"\n",
    "neuralop.models.base_model > BaseModel\n",
    "BaseModel에 _metadata가 들어있어서 load_dict 할때 오류가 나는 것이다.\n",
    "\"\"\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<bound method Module.state_dict of MultiHeadParametricFNO(\n",
      "  (param_encoder): Sequential(\n",
      "    (0): Linear(in_features=3, out_features=64, bias=True)\n",
      "    (1): ReLU()\n",
      "    (2): Linear(in_features=64, out_features=64, bias=True)\n",
      "  )\n",
      "  (trunk): FNO(\n",
      "    (positional_embedding): GridEmbeddingND()\n",
      "    (fno_blocks): FNOBlocks(\n",
      "      (convs): ModuleList(\n",
      "        (0-2): 3 x SpectralConv(\n",
      "          (weight): DenseTensor(shape=torch.Size([128, 128, 3]), rank=None)\n",
      "        )\n",
      "      )\n",
      "      (fno_skips): ModuleList(\n",
      "        (0-2): 3 x Flattened1dConv(\n",
      "          (conv): Conv1d(128, 128, kernel_size=(1,), stride=(1,), bias=False)\n",
      "        )\n",
      "      )\n",
      "      (channel_mlp): ModuleList(\n",
      "        (0-2): 3 x ChannelMLP(\n",
      "          (fcs): ModuleList(\n",
      "            (0): Conv1d(128, 64, kernel_size=(1,), stride=(1,))\n",
      "            (1): Conv1d(64, 128, kernel_size=(1,), stride=(1,))\n",
      "          )\n",
      "        )\n",
      "      )\n",
      "      (channel_mlp_skips): ModuleList(\n",
      "        (0-2): 3 x SoftGating()\n",
      "      )\n",
      "    )\n",
      "    (lifting): ChannelMLP(\n",
      "      (fcs): ModuleList(\n",
      "        (0): Conv1d(66, 256, kernel_size=(1,), stride=(1,))\n",
      "        (1): Conv1d(256, 128, kernel_size=(1,), stride=(1,))\n",
      "      )\n",
      "    )\n",
      "    (projection): ChannelMLP(\n",
      "      (fcs): ModuleList(\n",
      "        (0): Conv1d(128, 256, kernel_size=(1,), stride=(1,))\n",
      "        (1): Conv1d(256, 128, kernel_size=(1,), stride=(1,))\n",
      "      )\n",
      "    )\n",
      "  )\n",
      "  (heads): ModuleList(\n",
      "    (0-5): 6 x Sequential(\n",
      "      (0): Conv1d(128, 128, kernel_size=(1,), stride=(1,))\n",
      "      (1): GELU(approximate='none')\n",
      "      (2): BatchNorm1d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (3): Dropout(p=0.1, inplace=False)\n",
      "      (4): Conv1d(128, 64, kernel_size=(1,), stride=(1,))\n",
      "      (5): GELU(approximate='none')\n",
      "      (6): BatchNorm1d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (7): Dropout(p=0.1, inplace=False)\n",
      "      (8): Conv1d(64, 1, kernel_size=(1,), stride=(1,))\n",
      "    )\n",
      "  )\n",
      ")>\n"
     ]
    }
   ],
   "source": [
    "\n",
    "print(model.state_dict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Optimizer's state_dict:\n",
      "state \t {}\n",
      "param_groups \t [{'lr': 0.001, 'betas': (0.9, 0.999), 'eps': 1e-08, 'weight_decay': 0.0001, 'amsgrad': False, 'maximize': False, 'foreach': None, 'capturable': False, 'differentiable': False, 'fused': None, 'decoupled_weight_decay': True, 'params': [0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17, 18, 19, 20, 21, 22, 23, 24, 25, 26, 27, 28, 29, 30, 31, 32, 33, 34, 35, 36, 37, 38, 39, 40, 41, 42, 43, 44, 45, 46, 47, 48, 49, 50, 51, 52, 53, 54, 55, 56, 57, 58, 59, 60, 61, 62, 63, 64, 65, 66, 67, 68, 69, 70, 71, 72, 73, 74, 75, 76, 77, 78, 79, 80, 81, 82, 83, 84, 85, 86, 87, 88, 89, 90, 91, 92, 93, 94, 95]}]\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# print(\"Model's state_dict:\")\n",
    "# for param_tensor in model.state_dict():\n",
    "#     print(param_tensor, \"\\t\", model.state_dict()[param_tensor].size())\n",
    "    \n",
    "optimizer = torch.optim.AdamW(model.parameters(), lr=lr, weight_decay=weight_decay)\n",
    "\n",
    "print(\"Optimizer's state_dict:\")\n",
    "for var_name in optimizer.state_dict():\n",
    "    print(var_name, \"\\t\", optimizer.state_dict()[var_name])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "network_path = os.path.join(base_dir, \"fno_multihead.pth\")\n",
    "# 저장\n",
    "ckpt = {\n",
    "    \"state_dict\": model.state_dict(),\n",
    "    \"hparams\": {\n",
    "        \"param_embedding_dim\": param_embedding_dim,\n",
    "        \"fno_modes\": fno_modes,\n",
    "        \"fno_hidden_channels\": fno_hidden_channels,\n",
    "        \"n_layers\": n_layers,\n",
    "        \"shared_out_channels\": shared_out_channels,\n",
    "        \"n_params\": n_para,\n",
    "        \"n_heads\": n_rdc_coeffs,\n",
    "    },\n",
    "    # 전처리 스케일러도 같이 저장하면 편함\n",
    "    \"scaler_X_mean\": scaler_X.mean_, \"scaler_X_std\": scaler_X.scale_,\n",
    "    \"scalers_y_mean\": [s.mean_.ravel() for s in scalers_y],\n",
    "    \"scalers_y_std\":  [s.scale_.ravel() for s in scalers_y],\n",
    "}\n",
    "torch.save(ckpt, network_path)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/conda/lib/python3.11/site-packages/neuralop/layers/spectral_convolution.py:437: TracerWarning: Converting a tensor to a Python boolean might cause the trace to be incorrect. We can't record the data flow of Python values, so this value will be treated as a constant in the future. This means that the trace might not generalize to other inputs!\n",
      "  starts = [(max_modes - min(size, n_mode)) for (size, n_mode, max_modes) in zip(fft_size, self.n_modes, self.max_n_modes)]\n",
      "/opt/conda/lib/python3.11/site-packages/tltorch/factorized_tensors/factorized_tensors.py:66: UserWarning: Using a non-tuple sequence for multidimensional indexing is deprecated and will be changed in pytorch 2.9; use x[tuple(seq)] instead of x[seq]. In pytorch 2.9 this will be interpreted as tensor index, x[torch.tensor(seq)], which will result either in an error or a different result (Triggered internally at /pytorch/torch/csrc/autograd/python_variable_indexing.cpp:306.)\n",
      "  return self.__class__(self.tensor[indices])\n",
      "/opt/conda/lib/python3.11/site-packages/neuralop/layers/spectral_convolution.py:460: TracerWarning: Converting a tensor to a Python boolean might cause the trace to be incorrect. We can't record the data flow of Python values, so this value will be treated as a constant in the future. This means that the trace might not generalize to other inputs!\n",
      "  starts = [(size - min(size, n_mode)) for (size, n_mode) in zip(list(x.shape[2:]), list(weight.shape[weight_start_idx:]))]\n",
      "/opt/conda/lib/python3.11/site-packages/neuralop/layers/spectral_convolution.py:467: TracerWarning: Converting a tensor to a Python boolean might cause the trace to be incorrect. We can't record the data flow of Python values, so this value will be treated as a constant in the future. This means that the trace might not generalize to other inputs!\n",
      "  slices_x += [slice(None, -starts[-1]) if starts[-1] else slice(None)] # The last mode already has redundant half removed\n",
      "/opt/conda/lib/python3.11/site-packages/neuralop/layers/spectral_convolution.py:468: UserWarning: Using a non-tuple sequence for multidimensional indexing is deprecated and will be changed in pytorch 2.9; use x[tuple(seq)] instead of x[seq]. In pytorch 2.9 this will be interpreted as tensor index, x[torch.tensor(seq)], which will result either in an error or a different result (Triggered internally at /pytorch/torch/csrc/autograd/python_variable_indexing.cpp:306.)\n",
      "  out_fft[slices_x] = self._contract(x[slices_x], weight, separable=self.separable)\n",
      "/opt/conda/lib/python3.11/site-packages/opt_einsum/contract.py:317: TracerWarning: Converting a tensor to a Python integer might cause the trace to be incorrect. We can't record the data flow of Python values, so this value will be treated as a constant in the future. This means that the trace might not generalize to other inputs!\n",
      "  dim = int(sh[cnum])\n",
      "/opt/conda/lib/python3.11/site-packages/opt_einsum/parser.py:169: TracerWarning: Converting a tensor to a Python boolean might cause the trace to be incorrect. We can't record the data flow of Python values, so this value will be treated as a constant in the future. This means that the trace might not generalize to other inputs!\n",
      "  return tuple(max(shape[loc] for shape, loc in zip(shapes, [x.find(c) for x in inputs]) if loc >= 0) for c in output)\n"
     ]
    }
   ],
   "source": [
    "model = MultiHeadParametricFNO(\n",
    "    n_params=n_para,\n",
    "    param_embedding_dim=param_embedding_dim,\n",
    "    fno_modes=fno_modes,\n",
    "    fno_hidden_channels=fno_hidden_channels,\n",
    "    in_channels=1,\n",
    "    n_heads=n_rdc_coeffs,\n",
    "    n_layers=n_layers,\n",
    "    shared_out_channels=shared_out_channels\n",
    ").to(\"cpu\")\n",
    "ckpt = torch.load(network_path, map_location=\"cpu\", weights_only=False)\n",
    "sd = ckpt.get(\"state_dict\", ckpt); sd.pop(\"_metadata\", None)\n",
    "model.load_state_dict(sd, strict=False)\n",
    "model.eval().to(\"cpu\")\n",
    "\n",
    "# --- TorchScript trace (fix TracingCheckError by disabling graph re-check) ---\n",
    "L = int(grid_tensor.shape[0])  # 고정 길이(트레이스 시점에 고정됨)\n",
    "dummy_params = torch.zeros(1, int(X_tensor.shape[1]), dtype=torch.float32)  # [B, n_params]\n",
    "dummy_grid   = torch.zeros(1, L, 1, dtype=torch.float32)                     # [B, L, 1]\n",
    "\n",
    "# ts = torch.jit.trace(model, (dummy_params, dummy_grid))\n",
    "\n",
    "with torch.no_grad():\n",
    "    ts = torch.jit.trace(model, (dummy_params, dummy_grid), check_trace=False)\n",
    "\n",
    "network_path = os.path.join(base_dir, \"fno_multihead_ts.pt\")\n",
    "ts.save(network_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "model = MultiHeadParametricFNO(\n",
    "    n_params=n_para,\n",
    "    param_embedding_dim=param_embedding_dim,\n",
    "    fno_modes=fno_modes,\n",
    "    fno_hidden_channels=fno_hidden_channels,\n",
    "    in_channels=1,\n",
    "    n_heads=n_rdc_coeffs,\n",
    "    n_layers=n_layers,\n",
    "    shared_out_channels=shared_out_channels\n",
    ").to(device)\n",
    "\n",
    "optimizer = torch.optim.AdamW(model.parameters(), lr=lr, weight_decay=weight_decay)\n",
    "scheduler = torch.optim.lr_scheduler.CosineAnnealingLR(optimizer, T_max=epochs)\n",
    "model_save_path = os.path.join(base_dir, 'fno_seal_best_multihead.pth')\n",
    "\n",
    "for epoch in range(epochs):\n",
    "    model.train(); train_loss = 0.0\n",
    "    for params, functions in train_loader:\n",
    "        params, functions = params.to(device), functions.to(device)\n",
    "        batch_grid = grid_tensor.unsqueeze(0).repeat(params.size(0), 1, 1).to(device)\n",
    "        optimizer.zero_grad()\n",
    "        outputs = model(params, batch_grid)\n",
    "        loss = criterion(outputs, functions)\n",
    "        loss.backward(); optimizer.step()\n",
    "        train_loss += loss.item() * params.size(0)\n",
    "    model.eval(); val_loss = 0.0\n",
    "    with torch.no_grad():\n",
    "        for params, functions in val_loader:\n",
    "            params, functions = params.to(device), functions.to(device)\n",
    "            batch_grid = grid_tensor.unsqueeze(0).repeat(params.size(0), 1, 1).to(device)\n",
    "            outputs = model(params, batch_grid)\n",
    "            val_loss += criterion(outputs, functions).item() * params.size(0)\n",
    "    train_loss /= len(train_dataset); val_loss /= len(val_dataset)\n",
    "    if (epoch+1) % 50 == 0:\n",
    "        print(f'Epoch {epoch+1}/{epochs}, Train {train_loss:.6f}, Val {val_loss:.6f}')\n",
    "    if val_loss < best_val_loss:\n",
    "        best_val_loss = val_loss\n",
    "        torch.save({'state_dict': model.state_dict()}, model_save_path)\n",
    "    scheduler.step()\n",
    "    "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
